\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Normal Distributions}
\section{Density Functions}
The univariate probability density function is
\begin{equation}
    \label{eq:uninormaldens}
    f\left(x| \mu, \sigma \right) =
    \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x - \mu)^2}{2\sigma^2}}
\end{equation}
The univariate cumulative density function is
\begin{equation}
    \label{eq:uninormalcum}
    F\left(x| \mu, \sigma \right) =
    \frac{1}{2}\left[ 1 + erf\left(\frac{x - \mu}{\sigma \sqrt{2}}  \right) \right]
\end{equation}
The multivariate probability density function is given below.  In these
equations, bold-face type indicates either a column vector (lower-case) or a
matrix (upper-case).
\begin{align}
    \label{eq:multinormaldens}
    f\left(\B{x}| \B{\mu}, \B{\Sigma} \right)
    &= \frac{1}{
            |\B{\Sigma}|^\frac{1}{2} (2\pi)^{\left(\frac{k}{2}\right)}
        }
        e^{
            -\frac{1}{2}
            (\B{x} - \B{\mu})^T
            \B{\Sigma}^{-1}
            (\B{x} - \B{\mu})
    }\\
    &= \frac{|\B{\Lambda}|^\frac{1}{2}}{
             (2\pi)^{\left(\frac{k}{2}\right)}
        }
        e^{
            -\frac{1}{2}
            (\B{x} - \B{\mu})^T
            \B{\Lambda}
            (\B{x} - \B{\mu})
    }\nonumber
\end{align}
Where $\B{\Lambda} = \B{\Sigma^{-1}}$ is the precision
matrix, and $k$ is the dimensionality of the vector space.


\section{Adding Distributions}
The sum of $N$ normally distributed random variables is also normally
distributed with parameters
\begin{equation}
    \label{eq:sum_normal_mu_var}
    \B\mu = \sum\limits_{i=1}^{N}\B\mu_i
    \qquad\text{and}\qquad
    \B{\Sigma} = \sum\limits_{i=1}^{N}\B{\Sigma_i}
\end{equation}

\section{Multiplying Distributions}
The product of $N$ normal distributions is also a normal distribution with
parameters
\begin{equation}
    \label{eq:sum_normal_mu_var}
    \B{\Lambda} = \sum\limits_{i=1}^{N}\B{\Lambda_i}
    %\B\mu = \frac{
    %    \sum\limits_{i=1}^{N}\B{\Lambda_i\mu_i}
    %}{
    %    \sum\limits_{i=1}^{N}\B{\Lambda_i}
    %}
    \qquad\text{and}\qquad
    \B{\mu} = \B{\Lambda}^{-1}\sum\limits_{i=1}^{N}\B{\Lambda_i\mu_i}
\end{equation}
Where $\B{\Lambda} = \B{\Sigma^{-1}}$ is the precision
matrix.


\section{Incrementally Incorporating Data Into Distribution}
This section builds on a white paper by Raul Rojas entitled ``The Kalman
Filter.''  We'll start by considering the univariate normal distrubtion
and then generalize to the multivariate case. 

Assume you start with some initial values for $\mu_0$ and $\sigma_0$.  The
following recursive update equations can be used to incrementally update these
initial values to reflect all the values observed thus far.

\begin{align}
    \label{eq:normal_update_1d}
    \mu_{n+1} &= \frac{n}{n + 1} \mu_n  + \frac{1}{n+1}x_{n+1} \\
    \sigma_{n+1}^2 &= \frac{n}{n + 1}\sigma_n^2
        + \frac{n}{(n+1)^2} \left( x_{n+1} - \mu_n \right) ^ 2\nonumber
\end{align}

The following equavalent formulation is sometimes easier to think
about and will lead us towards handling the multivariate case.
\begin{align}
    \label{eq:normal_update_1d_with_gain}
    K &\equiv \frac{1}{n + 1}\\
    \mu_{n+1} &= \mu_n  + K \left( x_{n+1} - \mu_n \right)\nonumber\\
    {\sigma'}_n^2 &= \sigma_n^2 + K \left( x_{n+1} - \mu_n  \right)^2\nonumber\\
    \sigma_n^2 &= (1-K){\sigma'}_n^2\nonumber
\end{align}


The equations in \eqref{eq:normal_update_1d_with_gain} can be generalized to
obtain the updated multivariate mean, $\B{\mu}_{n+1}$ and covariance,
$\B{\Sigma}_{n+1}$.

\begin{align}
    \label{eq:normal_update_multi}
    K &\equiv \frac{1}{n + 1}\\
    \B{\mu}_{n+1} &= \B{\mu}_{n} +
                          K \left(
                              \B{x}_{n+1} - \B{\mu}_{n}
                          \right) \nonumber\nonumber\\
    {\B{\Sigma}'}_{n} &= \B{\Sigma}_{n}
                          + K\left( \B{x_{n+1}} - \B{\mu_n}  \right)
                          \left( \B{x_{n+1}} - \B{\mu_n} \right)^T\nonumber\\
    {\B{\Sigma}}_{n+1} &= (1 - K) {\B{\Sigma}'}_{n}\nonumber
\end{align}

\section{Incremental Updates with ``Forgetting''}
In this section, I describe one way of incrementally incorporating data into a
normal distribution while discounting older data.  This is very similar to what
a Kalman filter does, only quite a bit more simple.

The update equations \eqref{eq:normal_update_1d_with_gain}
\eqref{eq:normal_update_multi} in the previous section contain the ``gain''
factor, $K$, which depends on how many data points the distribution has already
ingested.  The gain factor is essentially a weighting factor indicating the
relative importance of the current data point with respect to all previously
observed points.  Now consider what happens if we initialize our distribution
with some $\mu_0$ and $\sigma_0$ and artificially freeze the number of points to
some value of our choosing, $n_{max}$.  We then continually feed the
distribution identical data points with some final value $x_f$ and watch how
the mean value changes.  The gain factor, of course, will be constant at
\begin{equation}
    \label{eq:k_const}
    K_{max} = \frac{1}{1 + n_{max}}.
\end{equation}
Then starting with the second line of
\eqref{eq:normal_update_1d_with_gain} we see
\begin{align}
    \label{eq:interest}
    \mu_{n+1} &= \mu_n  + K_{max} \left( x_{n+1} - \mu_n \right)\nonumber\\
              &= \mu_n (1 - K_{max}) + K_{max}x_f
\end{align}
and defining $A \equiv 1-K_{max}$ and $B \equiv K_{max} x_f$
we see that the update equation takes on a form which should be familiar to
anyone who has examined the amortization problem for compounded interest.
\begin{equation}
    \label{eq:interest}
    \mu_{n+1} = A \mu_n + B.
\end{equation}
We turn now to investigating the dynamics of this amortization-like equation
which should give you the added bonus of helping you understand your mortgage
payments.
\subsection{Exploring the Amortization Update Equation}
Here is some text about that.

 
\end{document}
