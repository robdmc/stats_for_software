\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Normal Distributions}

%=============================================================================
\section{Density Functions}
The univariate probability density function is
\begin{equation}
    \label{eq:uninormaldens}
    f\left(x| \mu, \sigma \right) =
    \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x - \mu)^2}{2\sigma^2}}
\end{equation}
The univariate cumulative density function is
\begin{equation}
    \label{eq:uninormalcum}
    F\left(x| \mu, \sigma \right) =
    \frac{1}{2}\left[ 1 + erf\left(\frac{x - \mu}{\sigma \sqrt{2}}  \right) \right]
\end{equation}
The multivariate probability density function is given below.  In these
equations, bold-face type indicates either a column vector (lower-case) or a
matrix (upper-case).
\begin{align}
    \label{eq:multinormaldens}
    f\left(\B{x}| \B{\mu}, \B{\Sigma} \right)
    &= \frac{1}{
            |\B{\Sigma}|^\frac{1}{2} (2\pi)^{\left(\frac{k}{2}\right)}
        }
        e^{
            -\frac{1}{2}
            (\B{x} - \B{\mu})^T
            \B{\Sigma}^{-1}
            (\B{x} - \B{\mu})
    }\\
    &= \frac{|\B{\Lambda}|^\frac{1}{2}}{
             (2\pi)^{\left(\frac{k}{2}\right)}
        }
        e^{
            -\frac{1}{2}
            (\B{x} - \B{\mu})^T
            \B{\Lambda}
            (\B{x} - \B{\mu})
    }\nonumber
\end{align}
Where $\B{\Lambda} = \B{\Sigma^{-1}}$ is the precision
matrix, and $k$ is the dimensionality of the vector space.


%=============================================================================
\section{Adding Distributions}
The sum of $N$ normally distributed random variables is also normally
distributed with parameters
\begin{equation}
    \label{eq:sumnormalmuvar}
    \B\mu = \sum\limits_{i=1}^{N}\B\mu_i
    \qquad\text{and}\qquad
    \B{\Sigma} = \sum\limits_{i=1}^{N}\B{\Sigma_i}
\end{equation}

%=============================================================================
\section{Multiplying Distributions}
The product of $N$ normal distributions is also a normal distribution with
parameters
\begin{equation}
    \label{eq:sumnormalmuvar}
    \B{\Lambda} = \sum\limits_{i=1}^{N}\B{\Lambda_i}
    %\B\mu = \frac{
    %    \sum\limits_{i=1}^{N}\B{\Lambda_i\mu_i}
    %}{
    %    \sum\limits_{i=1}^{N}\B{\Lambda_i}
    %}
    \qquad\text{and}\qquad
    \B{\mu} = \B{\Lambda}^{-1}\sum\limits_{i=1}^{N}\B{\Lambda_i\mu_i}
\end{equation}
Where $\B{\Lambda} = \B{\Sigma^{-1}}$ is the precision
matrix.


%=============================================================================
\section{Incrementally Incorporating Data Into Distribution}
This section builds on a white paper by Raul Rojas entitled ``The Kalman
Filter.''  We'll start by considering the univariate normal distrubtion
and then generalize to the multivariate case. 

Assume you start with some initial values for $\mu_0$ and $\sigma_0$.  The
following recursive update equations can be used to incrementally update these
initial values to reflect all the values observed thus far.

\begin{align}
    \label{eq:normalupdate1d}
    \mu_{n+1} &= \frac{n}{n + 1} \mu_n  + \frac{1}{n+1}x_{n+1} \\
    \sigma_{n+1}^2 &= \frac{n}{n + 1}\sigma_n^2
        + \frac{n}{(n+1)^2} \left( x_{n+1} - \mu_n \right) ^ 2\nonumber
\end{align}

The following equavalent formulation is sometimes easier to think
about and will lead us towards handling the multivariate case.
\begin{align}
    \label{eq:normalupdate1dwithgain}
    K &\equiv \frac{1}{n + 1}\\
    \mu_{n+1} &= \mu_n  + K \left( x_{n+1} - \mu_n \right)\nonumber\\
    {\sigma'}_n^2 &= \sigma_n^2 + K \left( x_{n+1} - \mu_n  \right)^2\nonumber\\
    \sigma_n^2 &= (1-K){\sigma'}_n^2\nonumber
\end{align}


The equations in \eqref{eq:normalupdate1dwithgain} can be generalized to
obtain the updated multivariate mean, $\B{\mu}_{n+1}$ and covariance,
$\B{\Sigma}_{n+1}$.

\begin{align}
    \label{eq:normalupdatemulti}
    K &\equiv \frac{1}{n + 1}\\
    \B{\mu}_{n+1} &= \B{\mu}_{n} +
                          K \left(
                              \B{x}_{n+1} - \B{\mu}_{n}
                          \right) \nonumber\nonumber\\
    {\B{\Sigma}'}_{n} &= \B{\Sigma}_{n}
                          + K\left( \B{x_{n+1}} - \B{\mu_n}  \right)
                          \left( \B{x_{n+1}} - \B{\mu_n} \right)^T\nonumber\\
    {\B{\Sigma}}_{n+1} &= (1 - K) {\B{\Sigma}'}_{n}\nonumber
\end{align}

%=============================================================================
\section{Incremental Updates with ``Forgetting''}
In this section, I describe one way of incrementally incorporating data into a
normal distribution while discounting older data.  This is very similar to what
a Kalman filter does, only quite a bit more simple.

The update equations \eqref{eq:normalupdate1dwithgain}
\eqref{eq:normalupdatemulti} in the previous section contain the ``gain''
factor, $K$, which depends on how many data points the distribution has already
ingested.  The gain factor is essentially a weighting factor indicating the
relative importance of the current data point with respect to all previously
observed points.  Now consider what happens if we initialize our distribution
with some $\mu_0$ and $\sigma_0$ and artificially freeze the number of points to
some value of our choosing, $n_{max}$.  We then continually feed the
distribution identical data points with some final value $x_f$ and watch how
the mean value changes.  The gain factor, of course, will be constant at
\begin{equation}
    \label{eq:kconst}
    K_{max} = \frac{1}{1 + n_{max}}.
\end{equation}
Then starting with the second line of
\eqref{eq:normalupdate1dwithgain} we see
\begin{align}
    \label{eq:interest}
    \mu_{n+1} &= \mu_n  + K_{max} \left( x_{n+1} - \mu_n \right)\nonumber\\
              &= \mu_n (1 - K_{max}) + K_{max}x_f
\end{align}
and defining $A \equiv 1-K_{max}$ and $B \equiv K_{max} x_f$
we see that the update equation takes on a form which should be familiar to
anyone who has examined the amortization problem for compounded interest.
\begin{equation}
    \label{eq:interest}
    \mu_{n+1} = A \mu_n + B.
\end{equation}
To see the correspondence, imagine that $n$ indexes the number of months you
are into your mortage, $\mu_n$ represents the outstanding balance on your
mortage, $A$ represents your monthly interest rate, and $B$ represents your
monthly payment (with a negative sign). 

%-----------------------------------------------------------------------------
\subsection{Exploring the Amortization Update Equation}
The amortization equation can apply to any system.  Above we will apply it to
the evolution of mean values, but it could apply to anything.  We therefore
reexpress the equation in terms of some general variable $x$.

\begin{equation}
    \label{eq:amortize}
    x_{n+1} = A x_n + B.
\end{equation}

Let's assume we start off with some initial value for $x$ that we will call
$x_0$, and then step through the first few iterations of equation
\eqref{eq:amortize}.

\begin{align}
    \label{eq:steps}
    x_0 &= x_0\nonumber\\
    x_1 &= A x_0 + B\nonumber\\
    x_2 &= A^2 x_0 + AB + B\nonumber\\
    x_3 &= A^3 x_0 + AB^2 + AB + B\nonumber\\
    ...\nonumber\\
    x_n &=
        A^n x_0 +
        B\sum\limits_{i=0}^{n-1}A^i
\end{align}
The summation in \eqref{eq:steps} occures quite frequently in different
analysis problems, and it has a closed-form solution as long as $A < 1$.

\begin{equation}
    \label{eq:summation}
    \sum\limits_{i=0}^{n-1}A^i = \frac{1 - A ^ N}{1 - A}
\end{equation}
We use this result to obtain the final result for our amortization problem.
\begin{align}
    \label{eq:amortizeresults}
    x_n &= A^n x_0 + \frac{B}{1 - A} - \frac{B A ^ n}{1 - A} \nonumber\\
        &= A^n \left( x_0 - \frac{B}{1 - A} \right) + \frac{B}{1 - A}
\end{align}

Notice that \eqref{eq:amortizeresults} is exponential in $n$.  This means
that the value will start at $x_0$ and exponentially approach the final value
$B / (1-A)$.  It is often more convenient to think about exponential processes
in terms of their expontential ``decay constant'' which with will label as
$n_d$ and compute with the following steps.
\begin{align}
    \label{eq:decay}
    A^n &= e^{-n / n_d} \nonumber\\
    ln(A) &= -\frac{1}{n_d} \nonumber\\
    n_d &= -\frac{1}{ln(A)}
\end{align}
This means that equation \eqref{eq:amortizeresults} can also be expressed as
\begin{equation}
    \label{eq:amortizeresultsexp}
    x_n = e^{-n/n_d}
          \left( x_0 - \frac{B}{1 - A} \right) + \frac{B}{1 - A}
\end{equation}

%-----------------------------------------------------------------------------
\subsection{Applying Amortization Results to ``Forgetting Problem''}
So let's pick up where we left of with the problem of incrementally updating a
normal distribution while ``forgetting'' old observations.  We had a recursive
update equation for the mean value given by
\begin{equation}
    \mu_{n+1} = A \mu_n + B. \nonumber
\end{equation}
where
\begin{align}
    A &= 1 - K_{max} \nonumber\\
    B &= x_f k_{max} \nonumber\\
    k_{max} &= \frac{1}{1+n_{max}}
\end{align}
I won't go through all the steps here, but applying the amortization results
to these equations will give you the following result.

\begin{equation}
    \label{eq:expsolution}
    \mu_n = e^{-ln \left( \frac{n_{max} + 1}{n_{max}}  \right) n}
          \left( x_0 - x_f \right) + x_f
\end{equation}

So let's say in words what we have shown here.  Placing an upper limit on the
number of points a normal distribution thinks it has observed will have the
following effects.  As long as the number of observations is less than the
limit, the normal distribution will behave perfectly ``normal'' and
continually become more and more precise in its estimate of the mean.  As soon
as the threshold is crossed though, it will subtly change its behavior and
begin to follow the general trends of the input data maintaining an estimate
of the current uncertainty.  It will exponentially forget older measurments
with a decay constant given by equation \eqref{eq:decay}. Although derived for
the univariate case, similar arguments can be made for the multivariate case.
 
%=============================================================================
\section{Condioning and Marginalizing}
One of the most common problems in statistics is to determine causation.
Basically people want to be able to answer the question ``If I change this
variable, how likely is it that this other variable will change, and by how
much?''.  In general, causation is hard to prove, but one ingredient for
causation is correlation.  If movement in one variable induces movement in
another, then they will be correlated.  The mulitvariate distribution provides
some nice tools for analyzing these correlations.

Recall that the multivariate normal distribution has a density function given
by
\begin{equation}
    f\left(\B{x}| \B{\mu}, \B{\Sigma} \right)
    = \frac{1}{
            |\B{\Sigma}|^\frac{1}{2} (2\pi)^{\left(\frac{k}{2}\right)}
        }
        e^{
            -\frac{1}{2}
            (\B{x} - \B{\mu})^T
            \B{\Sigma}^{-1}
            (\B{x} - \B{\mu})
    }
\end{equation}
Evaluation this function at a particular value for $\B{x}$ will tell you how
likely you are to observe a set of variables with values near $\B{x}$. But you
can then ask the question, ``If I know the value of the first element of
$\B{x}$, how does that change the distriubtion for all the rest of the values
in $\B{x}$?  This is called conditioning.

We will study conditioning by partioning the elements of $\B{x}$ into two
sets, the unkown set $\B{x_1}$ and the known set $\B{x_2}$.  What this means
is that we know the values for $\B{x_2}$ and want the updated probability
distribution for the unkown set $\B{x_1}$. Let's see how this works.  First I
carry out my partitioning
\begin{equation}
    \label{eq:xpartition}
    \B{x} = 
    \begin{bmatrix}
        \B{x_1} \\
        \B{x_2}
    \end{bmatrix}
\end{equation}
This then leads to corresponding partitions for the mean vector
\begin{equation}
    \label{eq:meanpartition}
    \B{\mu} = 
    \begin{bmatrix}
        \B{\mu_1} \\
        \B{\mu_2}
    \end{bmatrix}
\end{equation}
and covariance matrix
\begin{equation}
    \label{eq:meanpartition}
    \B{\Sigma} = 
    \begin{bmatrix}
        \B{\Sigma_{11}} & \B{\Sigma_{12}} \\ 
        \B{\Sigma_{21}} & \B{\Sigma_{22}} \\
    \end{bmatrix}
\end{equation}
Ignoring the normalization constant, this means I can rewrite the density
function as
\begin{equation}
    f\left(\B{x_1}, \B{x_2} \right)
     = e^{
        -\frac{1}{2}
        \begin{bmatrix}
            \B{x_1} - \B{\mu_1} \\
            \B{x_2} - \B{\mu_2}
        \end{bmatrix}^T
        \begin{bmatrix}
            \B{\Sigma_{11}} & \B{\Sigma_{12}} \\ 
            \B{\Sigma_{21}} & \B{\Sigma_{22}} \\
        \end{bmatrix}^{-1}
        \begin{bmatrix}
            \B{x_1} - \B{\mu_1} \\
            \B{x_2} - \B{\mu_2}
        \end{bmatrix}
     }.
\end{equation}
But what I really want to know is the conditional distrubtion
\begin{equation}
    f\left(\B{x_1}| \B{x_2}=\B{a} \right).
\end{equation}
The algebra required to get this result is super tedious and involves
something called the Schur complement, so I'm just going to state the result.
The conditional distriubtion is just another multivariate normal, 
\begin{equation}
    f\left(\B{x_1}| \B{x_2}=\B{a} \right) = 
        \frac{1}{
                    |\B{\bar\Sigma}|^\frac{1}{2} (2\pi)^{\left(\frac{k}{2}\right)}
                }
                e^{
                    -\frac{1}{2}
                    (\B{x_1} - \B{\bar\mu})^T
                    \B{\bar\Sigma}^{-1}
                    (\B{x_1} - \B{\bar\mu})
            }
\end{equation}
where
\begin{equation}
    \B{\bar{\mu}} = \B{\mu_1} + \B{\Sigma_{12}\Sigma_{22}^{-1}}
    \left( \B{a} - \B{\mu_2} \right)
\end{equation}
and
\begin{equation}
    \B{\bar{\Sigma}} =
        \B{\Sigma_{11}} - \B{\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}}
\end{equation}


















\end{document}
